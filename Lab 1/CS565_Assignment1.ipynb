{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS565_Assignment1",
      "provenance": [],
      "collapsed_sections": [
        "nIB_jVS2seK4",
        "9cgvm9VosYfG",
        "04WOyS9W5Ko3",
        "_dLDmucJ1Qsw",
        "LoeGcxX3hViV",
        "Amm16syPJIEn"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6v9OddTImWK"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIB_jVS2seK4"
      },
      "source": [
        "## Installing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "trOm_BCQyRfX"
      },
      "source": [
        "!pip install inltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6li2nNVbrzyp"
      },
      "source": [
        "import nltk\n",
        "import numpy\n",
        "import math\n",
        "\n",
        "from nltk.util import ngrams\n",
        "from matplotlib import pyplot\n",
        "from inltk.inltk import tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cgvm9VosYfG"
      },
      "source": [
        "## Loading The Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_ezyJPRscli"
      },
      "source": [
        "# The address has to change according to the location of file on the drive.\n",
        "en_path = '/content/drive/My Drive/Data/en_wiki.txt'\n",
        "hi_path = '/content/drive/My Drive/Data/hi_wiki.txt'\n",
        "\n",
        "# Note: Only 20% of the corpus is used in both cases, catering to the\n",
        "#       memory/time constraints of Google Colab Notebook.\n",
        "en_wiki = open(en_path,'r').read()\n",
        "en_wiki = en_wiki[0 : round(0.2*len(en_wiki))]\n",
        "\n",
        "hi_wiki = open(hi_path,'r').read()\n",
        "hi_wiki = hi_wiki[0 : round(0.2*len(hi_wiki))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xVUM282b6tv"
      },
      "source": [
        "# 1.3.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FWx889evSqv"
      },
      "source": [
        "## Sentence Segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgBE3Vpgzb-r"
      },
      "source": [
        "#### **English** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaDN3muj1G19"
      },
      "source": [
        "#Installation for using SpaCy library\n",
        "!pip install -U spacy\n",
        "!pip install -U spacy-lookups-data\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEKCGw2L2RAQ",
        "outputId": "e2b8a93c-a0f5-4fc0-c011-e24dd1925823",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# Using SpaCy Sentence Tokenizer\n",
        "import spacy\n",
        "\n",
        "def read_in_chunks(en_path):\n",
        "  fo = open(en_path, 'r')\n",
        "  while True:\n",
        "    data = fo.read(1000000)\n",
        "    if not data:\n",
        "      break\n",
        "    yield data\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "total_length = len(en_wiki)\n",
        "length = 0\n",
        "en_sent = []\n",
        "for data in read_in_chunks(en_path):\n",
        "    if (length + 1000000 > total_length): \n",
        "      break\n",
        "    length += len(data)\n",
        "    sentences = nlp(data) \n",
        "    en_sent += (sentence.text for sentence in sentences.sents)\n",
        "\n",
        "print(len(en_sent))\n",
        "print(en_sent[0 : 5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "180770\n",
            "['The word \"atom\" was coined by ancient Greek philosophers.', 'However, these ideas were founded in philosophical and theological reasoning rather than evidence and experimentation.', 'As a result, their views on what atoms look like and how they behave were incorrect.', 'They also could not convince everybody, so atomism was but one of a number of competing theories on the nature of matter.', 'It was not until the 19th century that the idea was embraced and refined by scientists, when the blossoming science of chemistry produced discoveries that only the concept of atoms could explain.\\n\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kClZf88_yyCT",
        "outputId": "6cdc7b87-a4c0-4c94-95f5-d22859976be3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "# Using NLTK Sentence Tokenizer\n",
        "en_sent = nltk.sent_tokenize(en_wiki)\n",
        "\n",
        "print(len(en_sent))\n",
        "print(en_sent[0 : 5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "176321\n",
            "['The word \"atom\" was coined by ancient Greek philosophers.', 'However, these ideas were founded in philosophical and theological reasoning rather than evidence and experimentation.', 'As a result, their views on what atoms look like and how they behave were incorrect.', 'They also could not convince everybody, so atomism was but one of a number of competing theories on the nature of matter.', 'It was not until the 19th century that the idea was embraced and refined by scientists, when the blossoming science of chemistry produced discoveries that only the concept of atoms could explain.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0sxM8QrzqK4"
      },
      "source": [
        "### **Hindi**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fz-o1JIGz4GD"
      },
      "source": [
        "!pip install stanza\n",
        "import stanza\n",
        "stanza.download('hi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IcUzAu1QRrh",
        "outputId": "e965592c-7bb1-4cf6-cdb0-78e5e28a284c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "# USING Stanza LIBRARY\n",
        "sent_tokenizer = stanza.Pipeline(lang='hi', processors='tokenize')\n",
        "\n",
        "hi_sent = sent_tokenizer(hi_wiki).sentences\n",
        "hi_sent = [sentence.text for sentence in hi_sent]\n",
        "\n",
        "print(len(hi_sent))\n",
        "print(hi_sent[0 : 5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-01 10:13:35 INFO: Loading these models for language: hi (Hindi):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | hdtb    |\n",
            "=======================\n",
            "\n",
            "2020-10-01 10:13:35 INFO: Use device: cpu\n",
            "2020-10-01 10:13:35 INFO: Loading: tokenize\n",
            "2020-10-01 10:13:35 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "72888\n",
            "['मास्टर ऑफ़ हेल्थ एडमिनिस्ट्रेशन या मास्टर ऑफ हेल्थकेयर एडमिनिस्ट्रेशन (एमएचए या एम.एच.ए) स्नातकोत्तर (पोस्ट ग्रेजुएशन) की एक पेशेवर डिग्री है जो स्वास्थ्य प्रशासन के क्षेत्र में दी जाती हैं।', 'यह उन छात्रों को प्रदान की जाती हैं जिन्होंने स्वास्थ्य प्रशासन, अस्पताल प्रबंधन एवं अन्य स्वास्थ्य सेवा संगठनों के क्षेत्र में जरूरी ज्ञान और दक्षता हासिल की हैं।', 'इन पाठ्यक्रमो में परिस्थितियों के अनुसार इनके सरंचना में अंतर हो सकता हैं', 'हालांकि व्यवसायी-शिक्षक मॉडल कार्यक्रम आमतौर पर चिकित्सा, स्वास्थ्य व्यवसायों या संबद्ध स्वास्थ्य के कॉलेजों में पाए जाते हैं, कक्षा-आधारित कार्यक्रम व्यवसाय या सार्वजनिक स्वास्थ्य के कॉलेजों में होते हैं।', 'इस पाठ्यक्रम के अध्ययन के दौरान आम तौर पर विद्यार्थियों को जनसंख्या स्वास्थ्य, स्वास्थ्य देखभाल अर्थशास्त्र, स्वास्थ्य नीति, संगठनात्मक व्यवहार, स्वास्थ्य से जुड़े संगठनों के प्रबंधन, स्वास्थ्य विपणन और संचार, मानव संसाधन प्रबंधन, सूचना प्रणाली प्रबंधन के अध्ययन एवं अन्य क्षेत्रों में व्यावहारिक अनुभव की भी आवश्यकता होती है।']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACh110lOTuSe"
      },
      "source": [
        "!pip install indic-nlp-library\n",
        "\n",
        "from indicnlp.tokenize import sentence_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnLjkNhtRYwu",
        "outputId": "1d3c7841-598b-4762-eedd-e90a55cb1188",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "#USING IndicNLP LIBRARY\n",
        "hi_sent = sentence_tokenize.sentence_split(hi_wiki, lang='hi')\n",
        "\n",
        "print(len(hi_sent))\n",
        "print(hi_sent[0 : 5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "68862\n",
            "['मास्टर ऑफ़ हेल्थ एडमिनिस्ट्रेशन या मास्टर ऑफ हेल्थकेयर एडमिनिस्ट्रेशन (एमएचए या एम. एच. ए) स्नातकोत्तर (पोस्ट ग्रेजुएशन) की एक पेशेवर डिग्री है जो स्वास्थ्य प्रशासन के क्षेत्र में दी जाती हैं।', 'यह उन छात्रों को प्रदान की जाती हैं जिन्होंने स्वास्थ्य प्रशासन, अस्पताल प्रबंधन एवं अन्य स्वास्थ्य सेवा संगठनों के क्षेत्र में जरूरी ज्ञान और दक्षता हासिल की हैं।', 'इन पाठ्यक्रमो में परिस्थितियों के अनुसार इनके सरंचना में अंतर हो सकता हैं हालांकि व्यवसायी-शिक्षक मॉडल कार्यक्रम आमतौर पर चिकित्सा, स्वास्थ्य व्यवसायों या संबद्ध स्वास्थ्य के कॉलेजों में पाए जाते हैं, कक्षा-आधारित कार्यक्रम व्यवसाय या सार्वजनिक स्वास्थ्य के कॉलेजों में होते हैं।', 'इस पाठ्यक्रम के अध्ययन के दौरान आम तौर पर विद्यार्थियों को जनसंख्या स्वास्थ्य, स्वास्थ्य देखभाल अर्थशास्त्र, स्वास्थ्य नीति, संगठनात्मक व्यवहार, स्वास्थ्य से जुड़े संगठनों के प्रबंधन, स्वास्थ्य विपणन और संचार, मानव संसाधन प्रबंधन, सूचना प्रणाली प्रबंधन के अध्ययन एवं अन्य क्षेत्रों में व्यावहारिक अनुभव की भी आवश्यकता होती है।', 'यह डिग्री प्रोग्राम स्वास्थ्य विषय के स्नातको को प्रबंधन के मुद्दों की विस्तृत एवं गहन समझ और उन्हें वरिष्ठ प्रबंधन भूमिकाओं के लिए तैयार करने के लिए डिज़ाइन किया गया है।']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykLnthLKyj5z"
      },
      "source": [
        "## Word Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eru6ncQjU16u"
      },
      "source": [
        "#### **English** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg_jVkTUVnDb",
        "outputId": "ad31622b-ee73-412c-c6e0-d45b3e83eece",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# USING NLTK LIBRARY - default method\n",
        "en_words = nltk.word_tokenize(en_wiki)\n",
        "\n",
        "print(len(en_words))\n",
        "print(en_words[0 : 10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4086209\n",
            "['The', 'word', '``', 'atom', \"''\", 'was', 'coined', 'by', 'ancient', 'Greek']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UP0eerx3ej3p",
        "outputId": "6ab76640-0990-4042-a596-e33e6aa85a81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# USING NLTK LIBRARY - Treebank method\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "word_tokenizer = TreebankWordTokenizer()\n",
        "en_words = word_tokenizer.tokenize(en_wiki)\n",
        "\n",
        "print(len(en_words))\n",
        "print(en_words[0 : 10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3927506\n",
            "['The', 'word', '``', 'atom', \"''\", 'was', 'coined', 'by', 'ancient', 'Greek']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTvFgVy9XiDR",
        "outputId": "0dd98229-c860-4ca9-bcd6-8d062050def2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# USING NLTK LIBRARY - Word Punctuation method\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "\n",
        "word_tokenizer = WordPunctTokenizer()\n",
        "en_words = word_tokenizer.tokenize(en_wiki)\n",
        "\n",
        "print(len(en_words))\n",
        "print(en_words[0 : 10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4458087\n",
            "['The', 'word', '\"', 'atom', '\"', 'was', 'coined', 'by', 'ancient', 'Greek']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIHmXu7rUwH6"
      },
      "source": [
        "### **Hindi**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddSLSM6fv17d",
        "outputId": "f7064eed-bef8-48e3-868c-a52c245553a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# USING NLTK LIBRARY\n",
        "hi_words = nltk.word_tokenize(hi_wiki)\n",
        "\n",
        "print(len(hi_words))\n",
        "print(hi_words[0 : 10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1633824\n",
            "['मास्टर', 'ऑफ़', 'हेल्थ', 'एडमिनिस्ट्रेशन', 'या', 'मास्टर', 'ऑफ', 'हेल्थकेयर', 'एडमिनिस्ट्रेशन', '(']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU0QLpxOZrY3",
        "outputId": "13a96ac8-1c8f-4a60-d998-5d2c7cd954ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "# USING Stanza LIBRARY\n",
        "hi_word_tokenizer = stanza.Pipeline(lang='hi', processors='tokenize', tokenize_no_ssplit=True)\n",
        "\n",
        "hi_sentences = hi_word_tokenizer(hi_wiki).sentences\n",
        "hi_words = []\n",
        "for _, sentence in enumerate(hi_sentences):\n",
        "  for token in sentence.tokens:\n",
        "    hi_words.append(token.text)\n",
        "\n",
        "print(len(hi_words))\n",
        "print(hi_words[0 : 10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-10-01 10:19:56 INFO: Loading these models for language: hi (Hindi):\n",
            "=======================\n",
            "| Processor | Package |\n",
            "-----------------------\n",
            "| tokenize  | hdtb    |\n",
            "=======================\n",
            "\n",
            "2020-10-01 10:19:56 INFO: Use device: cpu\n",
            "2020-10-01 10:19:56 INFO: Loading: tokenize\n",
            "2020-10-01 10:19:56 INFO: Done loading processors!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1681604\n",
            "['मास्टर', 'ऑफ़', 'हेल्थ', 'एडमिनिस्ट्रेशन', 'या', 'मास्टर', 'ऑफ', 'हेल्थकेयर', 'एडमिनिस्ट्रेशन', '(']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjyCmwSia2-L"
      },
      "source": [
        "from indicnlp.tokenize import indic_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyYSRLVWaf_w",
        "outputId": "42e7287e-7118-4fff-fd84-94cdf2a7f76e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "#USING IndicNLP LIBRARY\n",
        "hi_words = indic_tokenize.trivial_tokenize(hi_wiki)\n",
        "\n",
        "print(len(hi_words))\n",
        "print(hi_words[0 : 10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1729146\n",
            "['मास्टर', 'ऑफ़', 'हेल्थ', 'एडमिनिस्ट्रेशन', 'या', 'मास्टर', 'ऑफ', 'हेल्थकेयर', 'एडमिनिस्ट्रेशन', '(']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VcQbQ1T1frz"
      },
      "source": [
        "## Plotting N-Grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geTOUPEU3dPJ"
      },
      "source": [
        "def freq_plot(freq_dist, n, lang):\n",
        "    frequencies = [freq_dist[sample] for sample,_ in freq_dist.most_common(100)]\n",
        "    pos = numpy.arange(100)\n",
        "    \n",
        "    axes = pyplot.axes()\n",
        "\n",
        "    if (n == 1):\n",
        "      ngram_label = \"unigrams\"\n",
        "    elif (n == 2):\n",
        "      ngram_label = \"bigrams\"\n",
        "    elif (n == 3):\n",
        "      ngram_label = \"trigrams\"\n",
        "    \n",
        "    axes.set_xlabel(\"Rank\")\n",
        "    axes.set_ylabel(\"Frequency\")\n",
        "    axes.set_title(\"Frequency plot of {} by rank\".format(ngram_label))\n",
        "    axes.grid(True)\n",
        "\n",
        "    pyplot.ylim(0, max(frequencies) + 5)\n",
        "    pyplot.bar(pos, frequencies, 1.0, color='grey', edgecolor='black')\n",
        "    \n",
        "    figure = pyplot.gcf()\n",
        "    figure.savefig(lang + '_' + ngram_label, dpi=figure.dpi)\n",
        "    pyplot.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E1VFoIG01rQl"
      },
      "source": [
        "# This can be repeated for Hindi words\n",
        "en_unigram_dist = nltk.FreqDist(en_words)\n",
        "freq_plot(en_unigram_dist, 1, \"en\")\n",
        "\n",
        "en_bigram_dist  = nltk.FreqDist(list(ngrams(en_words, 2)))\n",
        "freq_plot(en_bigram_dist, 2, \"en\")\n",
        "\n",
        "en_trigram_dist = nltk.FreqDist(list(ngrams(en_words, 3)))\n",
        "freq_plot(en_trigram_dist, 3, \"en\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtZ15pjWLx6s",
        "outputId": "0e251b8e-f32e-4282-bc81-333088c2daaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "print(len(en_unigram_dist))\n",
        "print(en_unigram_dist.most_common(10))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "91903\n",
            "[('.', 291371), (',', 231646), ('the', 207921), ('of', 164211), ('and', 108203), ('%', 85282), ('was', 68873), ('in', 65428), ('a', 64016), ('to', 59192)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCUvCWkBhAL4"
      },
      "source": [
        "# 1.3.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yNnGKJGXT5AG"
      },
      "source": [
        "## Stemming Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hqzLv8hVX8i"
      },
      "source": [
        "### English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRf2QjW5VfkM"
      },
      "source": [
        "ps = PorterStemmer()\n",
        "en_stemmed_words = [ps.stem(word) for word in en_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSoAWDLzVcZ1"
      },
      "source": [
        "### Hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZWbAXX1vol9"
      },
      "source": [
        "def StemHindi(word):    \n",
        "    suffixes = {\n",
        "      1: [u\"ो\",u\"े\",u\"ू\",u\"ु\",u\"ी\",u\"ि\",u\"ा\"],\n",
        "      2: [u\"कर\",u\"ाओ\",u\"िए\",u\"ाई\",u\"ाए\",u\"ने\",u\"नी\",u\"ना\",u\"ते\",u\"ीं\",u\"ती\",u\"ता\",u\"ाँ\",u\"ां\",u\"ों\",u\"ें\"],\n",
        "      3: [u\"ाकर\",u\"ाइए\",u\"ाईं\",u\"ाया\",u\"ेगी\",u\"ेगा\",u\"ोगी\",u\"ोगे\",u\"ाने\",u\"ाना\",u\"ाते\",u\"ाती\",u\"ाता\",u\"तीं\",u\"ाओं\",u\"ाएं\",u\"ुओं\",u\"ुएं\",u\"ुआं\"],\n",
        "      4: [u\"ाएगी\",u\"ाएगा\",u\"ाओगी\",u\"ाओगे\",u\"एंगी\",u\"ेंगी\",u\"एंगे\",u\"ेंगे\",u\"ूंगी\",u\"ूंगा\",u\"ातीं\",u\"नाओं\",u\"नाएं\",u\"ताओं\",u\"ताएं\",u\"ियाँ\",u\"ियों\",u\"ियां\"],\n",
        "      5: [u\"ाएंगी\",u\"ाएंगे\",u\"ाऊंगी\",u\"ाऊंगा\",u\"ाइयाँ\",u\"ाइयों\",u\"ाइयां\"],\n",
        "    }\n",
        "\n",
        "    for L in range(5, 1, -1):\n",
        "      if len(word) > L:\n",
        "        for suffix in suffixes[L]:\n",
        "          if word.endswith(suffix):\n",
        "            return word[:-L]\n",
        "    return word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5QUngkB2oKr"
      },
      "source": [
        "hi_stemmed_words = [StemHindi(word) for word in hi_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04WOyS9W5Ko3"
      },
      "source": [
        "## Coverage Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHl8An8p5hls"
      },
      "source": [
        "def find_coverage(words, n, coverage) :\n",
        "    ngrams_list = list(ngrams(words, n))\n",
        "    freq_dist = nltk.FreqDist(ngrams_list)\n",
        "    freq_list = numpy.array(list(reversed(sorted([val for _, val in freq_dist.items()]))))\n",
        "    word_count = numpy.argmin(freq_list.cumsum() < freq_dist.N()*coverage)\n",
        "\n",
        "    return word_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcFOGKIL7KYa"
      },
      "source": [
        "# This can be repeated for Hindi words\n",
        "num_stem1 = find_coverage(en_stemmed_words, 1, 0.9)\n",
        "num_unstem1 = find_coverage(en_words, 1, 0.9)\n",
        "\n",
        "num_stem2 = find_coverage(en_stemmed_words, 2, 0.8)\n",
        "num_unstem2 = find_coverage(en_words, 2, 0.8)\n",
        "\n",
        "num_stem3 = find_coverage(en_stemmed_words, 3, 0.7)\n",
        "num_unstem3 = find_coverage(en_words, 3, 0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dLDmucJ1Qsw"
      },
      "source": [
        "# 1.3.3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEsQgmgnhdR-"
      },
      "source": [
        "## Applying Heuristics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gvpo05vLlYx"
      },
      "source": [
        "def en_regex_tokenizer(text):\n",
        "    pattern = r'''(?x)         \n",
        "          (?:[A-Z]\\.)+        # abbreviations, e.g. U.N.O.\n",
        "        | (?:\\s\\w\\w\\.)+       # titles, e.g. Dr., Ms.\n",
        "        | \\w+(?:-\\w+)*        # words with hyphens, e.g. zig-zag\n",
        "        | \\$?\\d+(?:\\.\\d+)?%?  # currency and percentages, e.g. $5, 25%\n",
        "        | \\.\\.\\.              # ellipsis, e.g. a,b,c...\n",
        "        | [][.,;\"'?():_`-]    # separate tokens, e.g. ], [\n",
        "    '''\n",
        "    \n",
        "    sentences = nltk.tokenize.RegexpTokenizer(pattern).tokenize(text)\n",
        "   \n",
        "    en_words = []\n",
        "    for sentence in sentences:\n",
        "      words = nltk.word_tokenize(sentence)\n",
        "      en_words += words\n",
        "    return en_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j18nrkOSMo9X"
      },
      "source": [
        "new_en_words = en_regex_tokenizer(en_wiki)\n",
        "new_en_words = [word.lower() for word in new_en_words]\n",
        "\n",
        "new_en_stemmed_words = [ps.stem(word) for word in new_en_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myh4tVCQU2c_"
      },
      "source": [
        "num_stem1 = find_coverage(new_en_stemmed_words, 1, 0.9)\n",
        "num_unstem1 = find_coverage(new_en_words, 1, 0.9)\n",
        "\n",
        "num_stem2 = find_coverage(new_en_stemmed_words, 2, 0.8)\n",
        "num_unstem2 = find_coverage(new_en_words, 2, 0.8)\n",
        "\n",
        "num_stem3 = find_coverage(new_en_stemmed_words, 3, 0.7)\n",
        "num_unstem3 = find_coverage(new_en_words, 3, 0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u-3-4d6anYX"
      },
      "source": [
        "## Likelihood Ratio Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TcqDGZGRkcKO"
      },
      "source": [
        "from math import log10\n",
        "\n",
        "def get_value(k, n, x):\n",
        "  return (log10(x)*k + log10(1-x)*(n-k))\n",
        "\n",
        "def constr_collocations(bigram_dist, unigram_dist, n):\n",
        "    collocation = []\n",
        "    i = 0\n",
        "    for bigram, freq in bigram_dist.items():\n",
        "        c12 = freq\n",
        "        c1 \t= unigram_dist[bigram[0]]\n",
        "        c2 \t= unigram_dist[bigram[1]]\n",
        "        \n",
        "        p \t= c2/n\n",
        "        p1 \t= c12/c1\n",
        "        p2 \t= (c2-c12)/(n-c1)\n",
        "\n",
        "        if(p2 == 0 or p1 == 0 or p == 0):\n",
        "            continue\n",
        "\n",
        "        if(p2 == 1 or p1 == 1 or p == 1):\n",
        "            continue\n",
        "\n",
        "        val  = get_value(c12, c1, p)\n",
        "        val += get_value(c2-c12, n-c1, p)\n",
        "        val -= get_value(c12, c1, p1)\n",
        "        val -= get_value(c2-c12, n-c1, p2)\n",
        "        val *= -2\n",
        "\n",
        "        if(val >= 7.88):\n",
        "            collocation += bigram\n",
        "\n",
        "    return collocation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bH6ZxuqGt2HG"
      },
      "source": [
        "from math import log10\n",
        "\n",
        "\n",
        "def getVal(k, n, x):\n",
        "  temp = log10(x) * k\n",
        "  temp2 = log10(1-x) * (n-k)\n",
        "  temp += temp2\n",
        "  return temp\n",
        "\n",
        "def construct_collocations(bigram_dist, unigram_dist, number_tokens):\n",
        "    collocation = []\n",
        "    i = 0\n",
        "    for bigram, freq in bigram_dist.items():\n",
        "        c12 = freq\n",
        "        c1 = unigram_dist[bigram[0]]\n",
        "        c2 = unigram_dist[bigram[1]]\n",
        "        n = number_tokens\n",
        "        p = c2/n\n",
        "        p1 = c12/c1\n",
        "        p2 = (c2 - c12)/(n-c1)\n",
        "        if(p2 == 0 or p1==0 or p==0):\n",
        "            continue\n",
        "\n",
        "        if(p2 == 1 or p1==1 or p==1):\n",
        "            continue\n",
        "\n",
        "        val = getVal(c12, c1, p) + getVal(c2 - c12, n-c1, p) - getVal(c12, c1, p1) - getVal(c2 - c12, n-c1, p2)\n",
        "        val *= -2\n",
        "\n",
        "        if(val >= 7.88):\n",
        "            collocation.append(bigram)\n",
        "\n",
        "    return collocation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-6BNGIkkkSI"
      },
      "source": [
        "en_collocations = constr_collocations(en_bigram_dist, en_unigram_dist,len(en_words))\n",
        "hi_collocations = constr_collocations(hi_bigram_dist, hi_unigram_dist,len(hi_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LoeGcxX3hViV"
      },
      "source": [
        "# 1.3.4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a22PJMeI49r"
      },
      "source": [
        "## Morphological Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1EjHIss-RZn"
      },
      "source": [
        "import random\n",
        "\n",
        "def random_freq_unigrams(freq_dist, count, set_count):\n",
        "  freq_unigrams = freq_dist.most_common(set_count)\n",
        "  return random.choices(freq_unigrams, k = count)\n",
        "\n",
        "def random_least_freq_unigrams(freq_dist, count, set_count):\n",
        "  least_freq_unigrams = freq_dist.most_common()[-set_count:]\n",
        "  return random.choices(least_freq_unigrams, k = count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ysQ4EU-_aKU"
      },
      "source": [
        "### English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79cRZUH1_fvn"
      },
      "source": [
        "!sudo apt-get install python-numpy libicu-dev\n",
        "!pip install PyICU polyglot pycld2 Morfessor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJj7LlUw_9OQ"
      },
      "source": [
        "from polyglot.downloader import downloader\n",
        "!polyglot download morph2.en\n",
        "from polyglot.text import Word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG0HRL2GAAaH"
      },
      "source": [
        "def morph_analysis(unigrams):\n",
        "  random_words = [word for word,_ in unigrams]\n",
        "  for word in random_words:\n",
        "    word = Word(word, language='en')\n",
        "    print(\"{} -> {}\".format(word, word.morphemes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQCT_aM3AuBj",
        "outputId": "c3771582-2fce-4b7b-de17-256bb10c8027",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "morph_analysis(random_freq_unigrams(en_unigram_dist, 5, 1000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "contains -> ['contain', 's']\n",
            "each -> ['e', 'ach']\n",
            "various -> ['vari', 'ous']\n",
            "as -> ['a', 's']\n",
            "being -> ['be', 'ing']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUAvoy3CBSsH",
        "outputId": "92459867-d70c-4b5a-ef95-ed5ddb6ff737",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "morph_analysis(random_least_freq_unigrams(en_unigram_dist, 5, 1000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "unscientific -> ['un', 'scientific']\n",
            "Mabhouh -> ['Ma', 'b', 'ho', 'u', 'h']\n",
            "Artery -> ['Arte', 'ry']\n",
            "incurring -> ['incur', 'ring']\n",
            "Qumsiyeh -> ['Qu', 'm', 's', 'i', 'y', 'eh']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqy6fEDQ_hTY"
      },
      "source": [
        "### Hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wacT514_71Z"
      },
      "source": [
        "!git clone https://github.com/anoopkunchukuttan/indic_nlp_resources.git\n",
        "!pip install indic-nlp-library"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_8TknyDCQEY"
      },
      "source": [
        "from indicnlp.morph import unsupervised_morph \n",
        "from indicnlp import common\n",
        "common.INDIC_RESOURCES_PATH=\"/content/indic_nlp_resources\"\n",
        "analyzer = unsupervised_morph.UnsupervisedMorphAnalyzer('hi')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pquRCTByCnr_"
      },
      "source": [
        "def morph_analysis(unigrams):\n",
        "  random_words = [word for word,_ in unigrams]\n",
        "  for word in random_words:\n",
        "    morpheme = analyzer.morph_analyze(word)\n",
        "    print(\"{} -> {}\".format(word, morpheme))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF3Bgrs-DD67",
        "outputId": "191039d9-be64-484b-d680-e060aa0cf3a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "morph_analysis(random_freq_unigrams(hi_unigram_dist, 5, 1000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "निर्धारित -> ['निर्धारित']\n",
            "करना -> ['कर', 'ना']\n",
            "सैन्य -> ['सैन्य']\n",
            "प्रसिद्ध -> ['प्रसिद्ध']\n",
            "राष्ट्रपति -> ['राष्ट्रपति']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAOhUjLzDEsT",
        "outputId": "cbc8ae9a-ce56-4181-e711-056d5a130199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "morph_analysis(random_least_freq_unigrams(hi_unigram_dist, 5, 1000))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "उपास्यदेव -> ['उपास्य', 'देव']\n",
            "'खैरागढ़ -> [\"'खैरागढ़\"]\n",
            "ख्+ -> ['ख्+']\n",
            "फ़िल्मी-हस्तियां -> ['फ़िल्मी-हस्तियां']\n",
            "रैंडर -> ['रैंड', 'र']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPx9A8EyhYGo"
      },
      "source": [
        "# 1.3.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kALOc2CJEY9"
      },
      "source": [
        "## Sub-Word Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZdMCIUzT__DU"
      },
      "source": [
        "### Training on Corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCTynvj5soqW"
      },
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "\n",
        "def build_vocab(freq_dist):\n",
        "  vocab = nltk.FreqDist([])\n",
        "  for word, freq in freq_dist.items():\n",
        "    w = ''\n",
        "    for c in word:\n",
        "      w += c + ' '\n",
        "    vocab[w +'$'] = freq\n",
        "  return vocab\n",
        "\n",
        "def get_stats(vocab):\n",
        "    pairs = defaultdict(int)\n",
        "    for word, frequency in vocab.items():\n",
        "        symbols = word.split()\n",
        "\n",
        "        for i in range(len(symbols) - 1):\n",
        "            pairs[symbols[i], symbols[i + 1]] += frequency\n",
        "    \n",
        "    return pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWKg7X2W0OfJ"
      },
      "source": [
        "def merge_vocab(pair, vocab):\n",
        "    new_vocab =  nltk.FreqDist([])\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    \n",
        "    for word in vocab:\n",
        "        w_out = pattern.sub(''.join(pair), word)\n",
        "        new_vocab[w_out] = vocab[word]\n",
        "\n",
        "    return new_vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zydnuvAWz_Aj"
      },
      "source": [
        "def train_corpus(freq_dist):\n",
        "  vocab = build_vocab(freq_dist)\n",
        "  fin_pairs = []\n",
        "\n",
        "  for i in range(500):\n",
        "    pairs = get_stats(vocab)\n",
        "    \n",
        "    if not pairs:\n",
        "        break\n",
        "\n",
        "    freq_pair = max(pairs, key=pairs.get)\n",
        "    fin_pairs.append(freq_pair)\n",
        "    vocab = merge_vocab(freq_pair, vocab)\n",
        "  \n",
        "  return (vocab, fin_pairs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgAaWtB3AF8a"
      },
      "source": [
        "### Testing on New Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFjt3c10CY5C"
      },
      "source": [
        "def merge_vocab_tokenize(pair, vocab):\n",
        "    new_vocab = []\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    pattern = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "   \n",
        "    for word in vocab:\n",
        "        w_out = pattern.sub(''.join(pair), word)\n",
        "        new_vocab.append(w_out)\n",
        "\n",
        "    return new_vocab\n",
        "\n",
        "def tokenize_corpus(corpus, fin_pairs):\n",
        "    words = [\" \".join(word) + \" $\" for word in corpus.split()]\n",
        "    for pair in fin_pairs:\n",
        "      words = merge_vocab_tokenize(pair, words)\n",
        "    return words "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Amm16syPJIEn"
      },
      "source": [
        "### English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vk9f1ajpDBzz",
        "outputId": "1064277b-0ac0-4f93-f788-163181d12820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "vocab, fin_pairs = train_corpus(en_unigram_dist)\n",
        "print(vocab.most_common(20))\n",
        "print(vocab.most_common()[-20:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('.$', 291371), (',$', 231646), ('the$', 207921), ('of$', 164211), ('and$', 108203), ('%$', 85282), ('was$', 68873), ('in$', 65428), ('a$', 64016), ('to$', 59192), ('were$', 54127), ('The$', 52467), ('\"$', 35371), ('is$', 34277), ('age$', 30634), ('for$', 29821), ('from$', 28787), ('-$', 27435), ('($', 26440), ('with$', 26127)]\n",
            "[('S z u c k o$', 1), ('C Q T s$', 1), ('S um mar iz ing$', 1), ('pre t ens e$', 1), ('P s y ch ol og ist s$', 1), ('P ol y g ra ph s$', 1), ('P T S D $', 1), ('h y p og l y c em ia$', 1), ('di sh on est y$', 1), ('un sc i ent if ic$', 1), ('vi ol at or s$', 1), ('V al id ity$', 1), ('M o y n i h an$', 1), ('S ec rec y$', 1), ('. . \" .$', 1), ('A s k ed$', 1), ('\" ` $', 1), ('pr ic k ing$', 1), ('B é land$', 1), ('su b j ec $', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhpXL7exDTuZ",
        "outputId": "0f064692-0abf-466c-8fcc-0b58d24dd528",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "new_corpus = 'friendly president makes statement capture meaning behind different emotions present'\n",
        "sub_word_tokens = tokenize_corpus(new_corpus, fin_pairs)\n",
        "print(sub_word_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fr i en d ly$', 'pre si d ent$', 'ma k es$', 'st at em ent$', 'cap tur e$', 'me an ing$', 'be h in d$', 'di ff er ent$', 'em ot ions$', 'present$']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "948GxO1FI-dk",
        "outputId": "9ab198fb-343d-4668-c573-3e62329a2258",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "new_corpus_words = new_corpus.split()\n",
        "for word in new_corpus_words:\n",
        "  word = Word(word, language=\"en\")\n",
        "  sub_word_tokens = ' + '.join(word.morphemes)\n",
        "  print(\"{} -> {}\".format(word, sub_word_tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "friendly -> friend + ly\n",
            "president -> president\n",
            "makes -> make + s\n",
            "statement -> state + ment\n",
            "capture -> capture\n",
            "meaning -> mean + ing\n",
            "behind -> be + hind\n",
            "different -> different\n",
            "emotions -> e + motion + s\n",
            "present -> present\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCUTQ-CoJLUY"
      },
      "source": [
        "### Hindi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9rPCwKLI5Nz",
        "outputId": "80431538-36c4-4783-b762-5da91bb5a5f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "vocab, fin_pairs = train_corpus(hi_unigram_dist)\n",
        "print(vocab.most_common(20))\n",
        "print(vocab.most_common()[-20:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('के$', 71492), ('में$', 54162), (',$', 53807), ('की$', 33545), ('और$', 31227), ('से$', 26883), ('का$', 24191), ('को$', 23815), ('है।$', 23771), ('है$', 21197), ('एक$', 15831), (')$', 13725), ('($', 13671), ('पर$', 12717), ('ने$', 11363), ('लिए$', 10580), ('भी$', 9482), ('हैं।$', 9384), ('किया$', 8923), (\"''$\", 8847)]\n",
            "[('ए ड ो आ र् ड$', 1), ('से गु इन$', 1), ('9 - वर् ष ीय$', 1), ('है । ज न् म जा त$', 1), ('क्र ी ज$', 1), ('। इन$', 1), ('वा यु मार् गों$', 1), ('ए प न िया$', 1), ('पै ट र् न । अ ट ला ं ट ै क् स िय ल$', 1), ('है ं - पु रु ष ों$', 1), ('5 0 - 6 9$', 1), ('20 - 3 5$', 1), ('1 0 - 3 0$', 1), ('है । 1 0$', 1), ('स्ट टर$', 1), ('है ं । वे$', 1), ('स् पै स् म$', 1), ('5 0 - 7 0$', 1), ('स् ट्र ै बि स् म स$', 1), ('के रा ट ो को न स$', 1)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P06gmwzLEy-",
        "outputId": "a93b1586-18b8-4fae-e8e1-371a279b4cce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "new_corpus = 'वर्तमान भविष्य विभाजित अद्भुत रचना खिलौने किताब विभिन्न भावनाएँ सिखाने'\n",
        "sub_word_tokens = tokenize_corpus(new_corpus, fin_pairs)\n",
        "print(sub_word_tokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['वर्त मान$', 'भ वि ष ्य$', 'वि भा ज ित$', 'अ द् भ ु त$', 'र च ना$', 'खि ल ौ ने$', 'कि ता ब$', 'वि भि न्न$', 'भा व ना ए ँ$', 'सि खा ने$']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN3mj2fuMVP-",
        "outputId": "ad39f243-380a-479e-b8c7-c87cc44a8543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "new_corpus_words = new_corpus.split()\n",
        "for word in new_corpus_words:\n",
        "  sub_word_tokens = ' + '.join(analyzer.morph_analyze(word))\n",
        "  print(\"{} -> {}\".format(word, sub_word_tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "वर्तमान -> वर्तमान\n",
            "भविष्य -> भविष्य\n",
            "विभाजित -> विभाजित\n",
            "अद्भुत -> अद्भुत\n",
            "रचना -> रचना\n",
            "खिलौने -> खिलौने\n",
            "किताब -> किताब\n",
            "विभिन्न -> विभिन्न\n",
            "भावनाएँ -> भावना + एँ\n",
            "सिखाने -> सिखा + ने\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}