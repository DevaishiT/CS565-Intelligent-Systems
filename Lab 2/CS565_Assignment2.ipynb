{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS565_Assignment2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NEj9ZPZkLat"
      },
      "source": [
        "**Following notebook contains the relevant code for Assignment 2, CS 565 - Intelligent Systems and Interfaces**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_Dl3qciWJKW"
      },
      "source": [
        "# Prequisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5cJyADCWEqG"
      },
      "source": [
        "# Mounting the google drive to directly access the dataset.\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdN9Jq59Wnqm"
      },
      "source": [
        "# NLTK will be used for word tokenization and sentence segmentation.\n",
        "!pip install inltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-IRg1MOYgcj"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import numpy as np\n",
        "import random\n",
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzA7ev2RXBI_"
      },
      "source": [
        "#Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6JyGNAuXHFo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "317a7d4c-1ba4-4ae5-a268-42047ae2c0c8"
      },
      "source": [
        "# The address has to change according to the location of file on the drive.\n",
        "path = '/content/drive/My Drive/Data/en_wiki.txt'\n",
        "en_wiki = open(path,'r').read()\n",
        "\n",
        "# Using NLTK Sentence Tokenizer\n",
        "dataset = nltk.sent_tokenize(en_wiki)\n",
        "\n",
        "print(len(dataset))\n",
        "print(dataset[0 : 5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "761582\n",
            "['The word \"atom\" was coined by ancient Greek philosophers.', 'However, these ideas were founded in philosophical and theological reasoning rather than evidence and experimentation.', 'As a result, their views on what atoms look like and how they behave were incorrect.', 'They also could not convince everybody, so atomism was but one of a number of competing theories on the nature of matter.', 'It was not until the 19th century that the idea was embraced and refined by scientists, when the blossoming science of chemistry produced discoveries that only the concept of atoms could explain.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC3CwERbaMoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da4a7ad-8b2f-46a5-976e-cf1c87b1cdac"
      },
      "source": [
        "# Shuffling the sentences in the dataset\n",
        "random.shuffle(dataset)\n",
        "print(dataset[0 : 5])\n",
        "\n",
        "# Splitting the dataset into training and testing data\n",
        "train_data = dataset[0 : round(0.9*len(dataset))]\n",
        "test_data = dataset[round(0.9*len(dataset)) : ]\n",
        "\n",
        "print(len(train_data), len(test_data))\n",
        "del dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Poets increasingly developed a self-conscious relationship to tradition, which took the form of a new emphasis on craftsmanship of expression and an idiosyncratic freedom in allusions to Classical and Biblical sources.', 'Dupin puts an advertisement in the newspaper asking if anyone has lost an \"Ourang-Outang\".', 'Lilies are usually planted as bulbs in the dormant season.', 'There is considerable commercial interest in the field because of its application to news-gathering, text categorization, voice-activation, archiving, and large-scale content-analysis.', 'He appeared again as Poirot in three made-for-television movies: \"Thirteen at Dinner\" (1985), \"Dead Man\\'s Folly\" (1986), and \"Murder in Three Acts\" (1986).']\n",
            "685424 76158\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdy7_7kHtLBi"
      },
      "source": [
        "# N-Gram Language Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQv5CkGz9ljX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96690fd2-5314-4ef7-ec02-5254a67ea070"
      },
      "source": [
        "# Creating 5 sets of training and development data\n",
        "train_data_list = []\n",
        "development_data_list = []\n",
        "\n",
        "for i in range(5):\n",
        "  random.shuffle(train_data)\n",
        "  train_data_list.append(train_data[0 : round(0.9*len(train_data))])\n",
        "  development_data_list.append(train_data[round(0.9*len(train_data)) : ])\n",
        "\n",
        "print(len(train_data_list[0]))\n",
        "print(len(development_data_list[0]))\n",
        "\n",
        "del train_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "616882\n",
            "68542\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPUWtb_4tued"
      },
      "source": [
        "# Finding out counts and list of unigram, bigrams and trigrams.\n",
        "# Note: The threshold is used to group very rare words into a seperate UNKNOWN category\n",
        "def ngram_train(sentences, threshold):\n",
        "  list_of_bigrams = []\n",
        "  list_of_trigrams = []\n",
        "\n",
        "  unigram_count = {}\n",
        "  bigram_count = {}\n",
        "  trigram_count = {}\n",
        "\n",
        "  for sentence in sentences:\n",
        "    words = sentence.split()\n",
        "    for i in range(len(words)):\n",
        "      \n",
        "      # If the word can be at the beginning of a bigram\n",
        "      if i < len(words)-1:\n",
        "        list_of_bigrams.append((words[i], words[i + 1]))\n",
        "        if (words[i], words[i + 1]) in bigram_count:\n",
        "          bigram_count[(words[i], words[i + 1])] += 1\n",
        "        else:\n",
        "          bigram_count[(words[i], words[i + 1])] = 1\n",
        "\n",
        "      # If the word can be at the beginning of a trigram\n",
        "      if i < len(words)-2:\n",
        "        list_of_trigrams.append((words[i], words[i + 1], words[i + 2]))\n",
        "        if (words[i], words[i + 1], words[i + 2]) in trigram_count:\n",
        "          trigram_count[(words[i], words[i + 1], words[i + 2])] += 1\n",
        "        else:\n",
        "          trigram_count[(words[i], words[i + 1], words[i + 2])] = 1\n",
        "      \n",
        "      if words[i] in unigram_count:\n",
        "        unigram_count[words[i]] += 1\n",
        "      else:\n",
        "        unigram_count[words[i]] = 1\n",
        "\n",
        "  unigram_count[\"unknown\"] = 0\n",
        "  for unigram in unigram_count:\n",
        "    if (unigram_count[unigram] < threshold):\n",
        "      unigram_count[\"unknown\"] += 1\n",
        "      unigram_count[unigram] = 0\n",
        "\n",
        "  return list_of_bigrams, list_of_trigrams, unigram_count, bigram_count, trigram_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Es4DtvwfLQAv"
      },
      "source": [
        "**Note that the below steps can be repeated for all the training data sets created by changing the index to 0, 1, 2, 3 and 4.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQIDJjao6uF_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d873bf23-2b64-492c-98e3-efd87e5d7529"
      },
      "source": [
        "# Calling ngram_train to find out the list and count of ngrams\n",
        "threshold = 4\n",
        "list_of_bigrams, list_of_trigrams, unigram_count, bigram_count, trigram_count = ngram_train(train_data_list[0], threshold)\n",
        "\n",
        "print(len(list_of_bigrams))\n",
        "print(list_of_bigrams[0:5])\n",
        "\n",
        "print(len(list_of_trigrams))\n",
        "print(list_of_trigrams[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12947650\n",
            "[('The', 'inward'), ('inward', 'code'), ('code', 'assists'), ('assists', 'in'), ('in', 'the')]\n",
            "12332527\n",
            "[('The', 'inward', 'code'), ('inward', 'code', 'assists'), ('code', 'assists', 'in'), ('assists', 'in', 'the'), ('in', 'the', 'delivery')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CkjlKewYcx2"
      },
      "source": [
        "# Assigns onegram probabilties to each element\n",
        "def calculate_onegram_prob(unigram_count):\n",
        "  onegram_probability = {}\n",
        "  total_onegram_count = 0\n",
        "\n",
        "  for onegram in unigram_count:\n",
        "    total_onegram_count += unigram_count[onegram]\n",
        "\n",
        "  for onegram in unigram_count:\n",
        "    onegram_probability[onegram] = unigram_count[onegram] / total_onegram_count\n",
        "\n",
        "  return onegram_probability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RyqrhvrbK5Y"
      },
      "source": [
        "# Assigns bigram probabilties to each element\n",
        "def calculate_bigram_prob(list_of_bigrams, unigram_count, bigram_count):\n",
        "  bigram_probability = {}\n",
        "\n",
        "  for bigram in list_of_bigrams:\n",
        "    if unigram_count.get(bigram[0]) == 0:\n",
        "      bigram_probability[bigram] = 0\n",
        "    else:\n",
        "      bigram_probability[bigram] = bigram_count[bigram] / (unigram_count.get(bigram[0]))\n",
        "\n",
        "  return bigram_probability\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4z8iLDMkbLIE"
      },
      "source": [
        "# Assigns bigram probabilties to each element\n",
        "def calculate_trigram_prob(list_of_trigrams, bigram_count, trigram_count):\n",
        "  trigram_probability = {}\n",
        "\n",
        "  for trigram in list_of_trigrams:\n",
        "    if bigram_count.get((trigram[0], trigram[1])) == 0:\n",
        "      trigram_probability[trigram] = 0\n",
        "    else:\n",
        "      trigram_probability[trigram] = trigram_count[trigram] / (bigram_count.get((trigram[0], trigram[1])))\n",
        "\n",
        "  return trigram_probability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJ1VEZ35OmAg"
      },
      "source": [
        "# Pre-computing probabilities for further sections\n",
        "onegram_prob = calculate_onegram_prob(unigram_count)\n",
        "bigram_prob  = calculate_bigram_prob(list_of_bigrams, unigram_count, bigram_count)\n",
        "trigram_prob = calculate_trigram_prob(list_of_trigrams, bigram_count, trigram_count)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4ivrtvNmmUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b8c88c-83ff-4b5c-d164-1388a049e82d"
      },
      "source": [
        "print(onegram_prob['the'])\n",
        "print(bigram_prob[('of', 'the')])\n",
        "print(trigram_prob[('value', 'of', 'the')])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.06507428053881525\n",
            "0.27801736915282826\n",
            "0.3549382716049383\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkdSigvghQIz"
      },
      "source": [
        "## Interpolation Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KskFGEZT-9qL"
      },
      "source": [
        "# Gets the counts of all the trigrams present in the test set\n",
        "def get_trigram_count(sentences):\n",
        "  trigram_count = {}\n",
        "  for sentence in sentences:\n",
        "    words = sentence.split()\n",
        "    for i in range(len(words) - 2):\n",
        "      if (words[i], words[i + 1], words[i + 2]) in trigram_count:\n",
        "        trigram_count[(words[i], words[i + 1], words[i + 2])] += 1\n",
        "      else:\n",
        "        trigram_count[(words[i], words[i + 1], words[i + 2])] = 1\n",
        "  return trigram_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0f4UmFfWbJK"
      },
      "source": [
        "# Returns the interpolated perplexity of the given list of sentences.\n",
        "def calculate_interpolation_lambda(sentences, test_trigram_count, onegram_prob, bigram_prob, trigram_prob, lambda_set):\n",
        "  likelihood = 0\n",
        "  \n",
        "  for sentence in sentences:\n",
        "    words = sentence.split()\n",
        "    for i in range(len(words) - 2):\n",
        "      trigram = (words[i], words[i + 1], words[i + 2])\n",
        "      if trigram in trigram_prob:\n",
        "        tri_prob = trigram_prob[trigram]\n",
        "      else:\n",
        "        tri_prob = 0\n",
        "\n",
        "      bigram = (words[i], words[i + 1])\n",
        "      if bigram in bigram_prob:\n",
        "        bi_prob = bigram_prob[bigram]\n",
        "      else:\n",
        "        bi_prob = 0\n",
        "\n",
        "      onegram = words[i]\n",
        "      if onegram in onegram_prob:\n",
        "        one_prob = onegram_prob[onegram]\n",
        "      else:\n",
        "        one_prob = 0\n",
        "\n",
        "      # print(one_prob, bi_prob, tri_prob)\n",
        "      prob = lambda_set[0]*one_prob + lambda_set[1]*bi_prob + lambda_set[2]*tri_prob\n",
        "      \n",
        "      if prob != 0:\n",
        "        # print(math.log(prob, 2))\n",
        "        likelihood += np.log2(prob)*test_trigram_count[trigram]\n",
        "\n",
        "  return likelihood"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db1AX7N4_wF8"
      },
      "source": [
        "# Returns the interpolated perplexity of the given list of sentences.\n",
        "def calculate_interpolation_perplexity(sentences, test_trigram_count, onegram_prob, bigram_prob, trigram_prob, lambda_set):\n",
        "  count = 0\n",
        "  likelihood = 0\n",
        "  \n",
        "  for sentence in sentences:\n",
        "    words = sentence.split()\n",
        "    for i in range(len(words) - 2):\n",
        "      trigram = (words[i], words[i + 1], words[i + 2])\n",
        "      if trigram in trigram_prob:\n",
        "        tri_prob = trigram_prob[trigram]\n",
        "      else:\n",
        "        tri_prob = 0\n",
        "\n",
        "      bigram = (words[i], words[i + 1])\n",
        "      if bigram in bigram_prob:\n",
        "        bi_prob = bigram_prob[bigram]\n",
        "      else:\n",
        "        bi_prob = 0\n",
        "\n",
        "      onegram = words[i]\n",
        "      if onegram in onegram_prob:\n",
        "        one_prob = onegram_prob[onegram]\n",
        "      else:\n",
        "        one_prob = 0\n",
        "\n",
        "      # print(one_prob, bi_prob, tri_prob)\n",
        "      prob = lambda_set[0]*one_prob + lambda_set[1]*bi_prob + lambda_set[2]*tri_prob\n",
        "      \n",
        "      if prob != 0:\n",
        "        # print(math.log(prob, 2))\n",
        "        \n",
        "        likelihood += math.log(prob, 2)*test_trigram_count[trigram]\n",
        "      count += 1\n",
        "\n",
        "  perplexity = math.pow(2, -1*(likelihood/count))\n",
        "  return perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfRjlvNB4A8d"
      },
      "source": [
        "**Note that the below steps can be repeated for all the corresponding development data sets created by changing the index to 0, 1, 2, 3 and 4.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK3O9HbFrkqz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b119c2d9-f81b-4ab2-d900-5711c027e758"
      },
      "source": [
        "test_trigram_count = get_trigram_count(development_data_list[0])\n",
        "\n",
        "max_lambda = []\n",
        "max_likelihood = float('-inf')\n",
        "\n",
        "# Iterate over lambda values to find the ideal parameters \n",
        "for lambda1 in range(1, 10, 1):\n",
        "  for lambda2 in range(1, 10-lambda1, 1):\n",
        "    lambda_set = [lambda1/10, lambda2/10, (10 - lambda1 - lambda2)/10]\n",
        "    likelihood = calculate_interpolation_lambda(development_data_list[0], test_trigram_count, onegram_prob, bigram_prob, trigram_prob, lambda_set)\n",
        "    \n",
        "    if (likelihood > max_likelihood):\n",
        "      max_lambda = lambda_set\n",
        "      max_likelihood = likelihood\n",
        "\n",
        "print(max_lambda, max_likelihood)\n",
        "\n",
        "perplexity = calculate_interpolation_perplexity(development_data_list[0], test_trigram_count, onegram_prob, bigram_prob, trigram_prob, max_lambda)\n",
        "print(perplexity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.8, 0.1, 0.1] -0.063324085910393\n",
            "1.043956568032204\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-J36FHTkCw0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cbaff4e-e510-4664-cb67-16e1c624559d"
      },
      "source": [
        "test_trigram_count = get_trigram_count(test_data)\n",
        "\n",
        "perplexity = calculate_interpolation_perplexity(test_data, test_trigram_count, onegram_prob, bigram_prob, trigram_prob, max_lambda)\n",
        "print(perplexity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.028583701239205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GfHlI80tfdL"
      },
      "source": [
        "# In case RAM isnt sufficient\n",
        "del test_trigram_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Gxs_dm6hMaE"
      },
      "source": [
        "## Laplace Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SZJyOIrZnM6p"
      },
      "source": [
        "# Gets the counts of all the trigrams present in the test set\n",
        "def get_trigram_count(sentences):\n",
        "  trigram_count = {}\n",
        "  for sentence in sentences:\n",
        "    words = sentence.split()\n",
        "    for i in range(len(words) - 2):\n",
        "      if (words[i], words[i + 1], words[i + 2]) in trigram_count:\n",
        "        trigram_count[(words[i], words[i + 1], words[i + 2])] += 1\n",
        "      else:\n",
        "        trigram_count[(words[i], words[i + 1], words[i + 2])] = 1\n",
        "  return trigram_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dlv14bIcl211"
      },
      "source": [
        "# Assigns onegram probabilties with laplace smoothing, we can change the value of k. \n",
        "def calculate_laplace_smoothing(list_of_trigrams, trigram_count):\n",
        "  k = 1\n",
        "  trigram_laplace_probability = {}\n",
        "  \n",
        "  for trigram in list_of_trigrams:\n",
        "    trigram_laplace_probability[trigram] = (trigram_count[trigram]+k) / (bigram_count.get((trigram[0], trigram[1])) + k*len(list_of_trigrams))\n",
        "\n",
        "  return trigram_laplace_probability"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0ZmD_1VihNj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44a6f053-d1a6-46af-e9ba-105df42ab7a1"
      },
      "source": [
        "trigram_laplace_prob = calculate_laplace_smoothing(list_of_trigrams, trigram_count)\n",
        "print(trigram_laplace_prob[('He', 'is', 'the')])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8.513323919488928e-06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAy6-qBjFgVf"
      },
      "source": [
        "def calculate_laplace_perplexity(test_trigram_count, trigram_laplace_prob):\n",
        "  likelihood = 0\n",
        "  for trigram in test_trigram_count:\n",
        "    if trigram in trigram_laplace_prob:\n",
        "      likelihood += test_trigram_count[trigram] * np.log2(trigram_laplace_prob[trigram])\n",
        "\n",
        "  count = 0\n",
        "  for trigram in test_trigram_count:\n",
        "    count += test_trigram_count[trigram]\n",
        "  \n",
        "  likelihood = likelihood / count\n",
        "  return pow(2,-1*likelihood), likelihood"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFFJHfV7JOD9"
      },
      "source": [
        "**Note that the below steps can be repeated for all the corresponding development data sets created by changing the index to 0, 1, 2, 3 and 4.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0PCS9m8nlGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8f671f2-8105-430b-aa8e-4cb331af93b0"
      },
      "source": [
        "test_trigram_count = get_trigram_count(development_data_list[0])\n",
        "\n",
        "perplexity, likelihood = calculate_laplace_perplexity(test_trigram_count, trigram_laplace_prob)\n",
        "print(perplexity, likelihood)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "177.01362939772972 -7.467716636566069\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5ttYU_aG60J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7afe5baa-7d57-40e7-c4d5-67ff274e1e46"
      },
      "source": [
        "test_trigram_count = get_trigram_count(test_data)\n",
        "\n",
        "perplexity, likelihood = calculate_laplace_perplexity(test_trigram_count, trigram_laplace_prob)\n",
        "print(perplexity, likelihood)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "179.26432942256946 -7.485944634563072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRmXWCbntEIS"
      },
      "source": [
        "# In case RAM isnt sufficient\n",
        "del trigram_laplace_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEN_aItLhkP8"
      },
      "source": [
        "## Discount Smoothing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2iW7Bw75CSD"
      },
      "source": [
        "# Gets the counts of all the unigrams present in the test set\n",
        "def get_unigram_count(sentences):\n",
        "  unigram_count = {}\n",
        "  for sentence in sentences:\n",
        "    words = sentence.split()\n",
        "    for i in range(len(words)):\n",
        "      if words[i] in unigram_count:\n",
        "        unigram_count[words[i]] += 1\n",
        "      else:\n",
        "        unigram_count[words[i]] = 1\n",
        "  return unigram_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe9vWLb6V_gs"
      },
      "source": [
        "# Gets the counts of all the bigrams present in the test set\n",
        "def get_bigram_count(sentences):\n",
        "  bigram_count = {}\n",
        "  for sentence in sentences:\n",
        "    words = sentence.split()\n",
        "    for i in range(len(words) - 1):\n",
        "      if (words[i], words[i + 1]) in bigram_count:\n",
        "        bigram_count[(words[i], words[i + 1])] += 1\n",
        "      else:\n",
        "        bigram_count[(words[i], words[i + 1])] = 1\n",
        "  return bigram_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxrGRvlg3Rl8"
      },
      "source": [
        "# Gets the counts of all the trigrams present in the test set\n",
        "def get_trigram_count(sentences):\n",
        "  trigram_count = {}\n",
        "  for sentence in sentences:\n",
        "    words = sentence.split()\n",
        "    for i in range(len(words) - 2):\n",
        "      if (words[i], words[i + 1], words[i + 2]) in trigram_count:\n",
        "        trigram_count[(words[i], words[i + 1], words[i + 2])] += 1\n",
        "      else:\n",
        "        trigram_count[(words[i], words[i + 1], words[i + 2])] = 1\n",
        "  return trigram_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "palWDNmxzbXP"
      },
      "source": [
        "# Finding out probabilities corresponding to a particular value of beta\n",
        "def calculate_bigram_discounting_beta(unigram_count, bigram_count, beta, unigram_prob):\n",
        "  current_prob = {}\n",
        "\n",
        "  for i in unigram_count:\n",
        "    sum_prob = 0\n",
        "    sigms = 0\n",
        "\n",
        "    for j in unigram_count:\n",
        "      if bigram_count[(i, j)] != 0:\n",
        "        current_prob[(i, j)] = (bigram_count[(i, j)] - beta)/ unigram_count[i]\n",
        "        sum_prob += current_prob[(i, j)]\n",
        "      else:\n",
        "        sigms += unigram_prob[j]\n",
        "\n",
        "    alpha = 1 - sum_prob\n",
        "    for j in unigram_count:\n",
        "      if bigram_count[(i, j)] == 0:\n",
        "        current_prob[(i, j)] = alpha*(unigram_prob[j] / sigms)\n",
        "\n",
        "  return current_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYMF6kSCtu0h"
      },
      "source": [
        "# Find out the parameter and implement discounting smoothing for bigrams.\n",
        "def calculate_bigram_discounting(unigram_count, bigram_count, unigram_prob, test_bigram_count):\n",
        "  bigram_probability = {}\n",
        "  max_likelihood = float('-inf')\n",
        "  max_beta = 0\n",
        "\n",
        "  for beta in range(10):\n",
        "    beta = beta/10\n",
        "    likelihood = 0\n",
        "    current_prob = calculate_bigram_discounting_beta(unigram_count, bigram_count, beta, unigram_prob)\n",
        "\n",
        "    for bigram in test_bigram_count:\n",
        "      likelihood += test_bigram_count[bigram]*np.log2(current_prob[bigram])\n",
        "\n",
        "    if likelihood > max_likelihood:\n",
        "      max_beta = beta\n",
        "      max_likelihood = likelihood\n",
        "      bigram_probability = current_prob\n",
        "\n",
        "  return max_beta, bigram_probability\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TgOlF7dvwvk"
      },
      "source": [
        "# Finding out probabilities corresponding to a particular value of beta\n",
        "def calculate_trigram_discounting_beta(unigram_count, bigram_count, trigram_count, beta, bigram_prob):\n",
        "  current_prob = {}\n",
        "\n",
        "  for i in unigram_count:\n",
        "    for j in unigram_count:\n",
        "      sum_prob = 0\n",
        "      sigms = 0\n",
        "\n",
        "      for k in unigram_count:\n",
        "        trigram = (i, j, k)\n",
        "\n",
        "        if trigram_count[trigram] != 0:\n",
        "          current_prob[trigram] = (trigram_count[trigram] - beta)/ bigram_count[(i, j)]\n",
        "          sum_prob += current_prob[trigram]\n",
        "        else:\n",
        "          sigms += bigram_prob[(j, k)]\n",
        "\n",
        "      alpha = 1 - sum_prob\n",
        "      for k in unigram_count:\n",
        "        if trigram_count[trigram] == 0:\n",
        "          current_prob[trigram] = alpha*(bigram_prob[(j, k)] / sigms)\n",
        "  \n",
        "  return current_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnJ9oFl7zV2b"
      },
      "source": [
        "# Find out the parameter and implement discounting smoothing for trigrams.\n",
        "def calculate_trigram_discounting(unigram_count, bigram_count, trigram_count, bigram_prob, test_trigram_count):\n",
        "  trigram_probability = {}\n",
        "  max_likelihood = float('-inf')\n",
        "  max_beta = 0\n",
        "\n",
        "  for beta in range(10):\n",
        "    beta = beta/10\n",
        "    likelihood = 0\n",
        "    current_prob = calculate_trigram_discounting_beta(unigram_count, bigram_count, trigram_count, beta, bigram_prob)\n",
        "\n",
        "    for trigram in test_trigram_count:\n",
        "      likelihood += test_trigram_count[trigram]*np.log2(current_prob[trigram])\n",
        "\n",
        "    if likelihood > max_likelihood:\n",
        "      max_beta = beta\n",
        "      max_likelihood = likelihood\n",
        "      trigram_probability = current_prob\n",
        "\n",
        "  return max_beta, trigram_probability\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtrpBVH03ZiY"
      },
      "source": [
        "def calculate_discounting_perplexity(test_unigram_count, test_trigram_count, trigram_prob):\n",
        "  likelihood = 0\n",
        "  for trigram in test_trigram_count:\n",
        "    count = test_trigram_count[trigram]\n",
        "    if trigram_prob[trigram] != 0:\n",
        "      likelihood += count*np.log2(trigram_prob[trigram])\n",
        "  \n",
        "  num = 0\n",
        "  for unigram in test_unigram_count:\n",
        "    num += test_unigram_count[unigram]\n",
        "  likelihood = likelihood / num\n",
        "\n",
        "  return pow(2,-1*likelihood), likelihood"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAlfKzbs7EWF"
      },
      "source": [
        "**Note that the below steps can be repeated for all the corresponding development data sets created by changing the index to 0, 1, 2, 3 and 4.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfYP9S6J5rbX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8aec6c03-23c8-4223-9f1a-f90b2c4199df"
      },
      "source": [
        "test_unigram_count = get_unigram_count(development_data_list[0])\n",
        "test_bigram_count = get_bigram_count(development_data_list[0])\n",
        "test_trigram_count = get_trigram_count(development_data_list[0])\n",
        "\n",
        "beta_bigram, bigram_prob = calculate_bigram_discounting(unigram_count, bigram_count, onegram_prob, test_bigram_count)\n",
        "beta_trigram, trigram_prob = calculate_trigram_discounting(unigram_count, bigram_count, trigram_count, bigram_prob, test_trigram_count)\n",
        "\n",
        "perplexity, likelihood = calculate_discounting_perplexity(test_unigram_count, test_trigram_count, trigram_prob)\n",
        "print(perplexity, likelihood)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11.85167084255618 -3.566921492304502\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}